{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 19:04:47.265280: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-09 19:04:47.347405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-09 19:04:47.386203: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-09 19:04:47.397439: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-09 19:04:47.456223: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-09 19:04:48.281155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()  # Register the class\n",
    "class MicroF2Score(tf.keras.metrics.Metric):\n",
    "    def _init_(self, name='micro_f2_score', beta=2, **kwargs):\n",
    "        super(MicroF2Score, self)._init_(name=name, **kwargs)\n",
    "        self.beta = beta\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Threshold y_pred to get binary predictions\n",
    "        y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "        \n",
    "        # Cast y_true to float32 to ensure compatibility\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        true_positive = tf.reduce_sum(y_true * y_pred)\n",
    "        false_positive = tf.reduce_sum(y_pred * (1 - y_true))\n",
    "        false_negative = tf.reduce_sum((1 - y_pred) * y_true)\n",
    "\n",
    "        # Update the corresponding weights\n",
    "        self.tp.assign_add(true_positive)\n",
    "        self.fp.assign_add(false_positive)\n",
    "        self.fn.assign_add(false_negative)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
    "        f_beta = (1 + self.beta*2) * (precision * recall) / (self.beta*2 * precision + recall + tf.keras.backend.epsilon())\n",
    "        return f_beta\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        self.fn.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731159289.358037   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.434475   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.437525   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.442754   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.448120   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.451285   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.622897   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.625208   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731159289.628175   48436 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-09 19:04:49.630446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3161 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y: (198982, 1400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 19:04:53.473085: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "\n",
    "    model = tf.keras.models.load_model('model.keras')\n",
    "\n",
    "    X = np.load('embeddings_1.npy')\n",
    "    X_2 = np.load('embeddings_2.npy')\n",
    "    X = np.concatenate((X, X_2), axis=0)\n",
    "    # Step 1: Read label data from files (assuming you have already defined this part)\n",
    "    label_data = []\n",
    "    file_names = ['icd_codes_1.txt', 'icd_codes_2.txt']  # Update with actual filenames\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as file:\n",
    "            label_data.extend(line.strip() for line in file if line.strip())\n",
    "\n",
    "    # Step 2: Create a set of unique ICD-10 codes for efficient lookup\n",
    "    unique_codes = set()\n",
    "    for labels in label_data:\n",
    "        unique_codes.update(labels.split(\";\"))\n",
    "    unique_codes = sorted(unique_codes)  # Convert to a sorted list at the end\n",
    "\n",
    "    # Step 3: Initialize the StringLookup layer\n",
    "    lookup_layer = tf.keras.layers.StringLookup(vocabulary=unique_codes, output_mode=\"multi_hot\", mask_token=None,num_oov_indices=0)\n",
    "\n",
    "    # Step 4: Create a tf.data.Dataset to handle large data efficiently\n",
    "    label_data_ds = tf.data.Dataset.from_tensor_slices(label_data)\n",
    "    # Step 5: Define a function to encode each label set\n",
    "    def encode_labels(labels):\n",
    "        multi_hot = lookup_layer(tf.strings.split(labels, sep=\";\"))\n",
    "        return tf.cast(multi_hot, dtype=tf.int16)  # Reducing the precision here\n",
    "    # Step 6: Map encoding function over the dataset and batch it\n",
    "    # Batch processing reduces memory usage\n",
    "    multi_hot_labels_ds = label_data_ds.map(encode_labels, num_parallel_calls=tf.data.AUTOTUNE).batch(1000)\n",
    "\n",
    "    # Step 7: Concatenate all batches to get the final `y` tensor\n",
    "    y = tf.concat(list(multi_hot_labels_ds), axis=0)\n",
    "\n",
    "    # Ensure the correct shape of `y`\n",
    "    print(\"Shape of y:\", y.shape)  # Should output: (200000, 1400)\n",
    "\n",
    "    y = y.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# with tf.device('/device:CPU:0'):\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import fbeta_score\n",
    "\n",
    "# # Function to find optimal threshold for each label\n",
    "# def tune_thresholds(y_true, y_pred, beta=2.0, num_thresholds=50):\n",
    "#     best_thresholds = []\n",
    "#     for i in range(y_true.shape[1]):\n",
    "#         best_f2 = 0\n",
    "#         best_threshold = 0.5\n",
    "#         for threshold in np.linspace(0.1, 0.9, num_thresholds):\n",
    "#             y_pred_bin = (y_pred[:, i] >= threshold).astype(int)\n",
    "#             f2 = fbeta_score(y_true[:, i], y_pred_bin, beta=beta, average='micro', zero_division=1)\n",
    "#             if f2 > best_f2:\n",
    "#                 best_f2 = f2\n",
    "#                 best_threshold = threshold\n",
    "#         best_thresholds.append(best_threshold)\n",
    "#     return best_thresholds\n",
    "\n",
    "# # Inference function with optimized thresholds for test data\n",
    "# def predict_with_thresholds(model, X, thresholds):\n",
    "#     y_pred = model.predict(X)\n",
    "#     y_pred_bin = np.array([(y_pred[:, i] >= thresholds[i]).astype(int) for i in range(y_pred.shape[1])]).T\n",
    "#     return y_pred_bin\n",
    "\n",
    "# with tf.device('/device:CPU:0'):\n",
    "#     # Get predictions on the validation set\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     # Find optimal thresholds for each label\n",
    "#     best_thresholds = tune_thresholds(y_val, y_val_pred)\n",
    "\n",
    "#     # Apply optimized thresholds to validation predictions\n",
    "#     y_val_pred_bin = np.array([(y_val_pred[:, i] >= best_thresholds[i]).astype(int) for i in range(y_val_pred.shape[1])]).T\n",
    "\n",
    "#     # Calculate and print the validation F2 score with optimized thresholds\n",
    "#     validation_f2 = fbeta_score(y_val, y_val_pred_bin, beta=2, average='micro', zero_division=1)\n",
    "#     print(\"Optimized Micro-F2 Score on validation set:\", validation_f2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  28/3110\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731159294.068067   48604 service.cc:146] XLA service 0x7fe99800a220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731159294.068098   48604 service.cc:154]   StreamExecutor device (0): Host, Default Version\n",
      "2024-11-09 19:04:54.081015: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1731159294.172145   48604 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3110/3110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    # Load best model\n",
    "    # model = tf.keras.models.load_model('best_model.keras')\n",
    "    # Load test data\n",
    "    X_test = np.load('test_data.npy')\n",
    "\n",
    "    # Predict probabilities and convert to binary using the tuned thresholds\n",
    "    # y_test_pred = predict_with_thresholds(model, X_test, best_thresholds)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Prepare reverse lookup\n",
    "    lookup_layer = tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_codes, invert=True, output_mode=\"int\", mask_token=None, num_oov_indices=0\n",
    "    )\n",
    "\n",
    "    # Convert predictions to ICD10 codes\n",
    "    predicted_indices = [np.where(pred_row == 1)[0] for pred_row in y_test_pred]\n",
    "    predicted_codes = [lookup_layer(indices).numpy() for indices in predicted_indices]\n",
    "    predicted_codes = [[code.decode('utf-8') for code in row] for row in predicted_codes]\n",
    "    predicted_labels = [';'.join(row) for row in predicted_codes]\n",
    "\n",
    "    # Create and save submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': range(1, len(predicted_labels) + 1),\n",
    "        'labels': predicted_labels\n",
    "    })\n",
    "    submission_df.to_csv('submission_srini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
