{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.load('embeddings_1.npy')\n",
    "X_2 = np.load('embeddings_2.npy')\n",
    "X = np.concatenate((X, X_2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions: (198982, 1024)\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(X)           # Number of rows\n",
    "num_columns = len(X[0])     # Number of columns (assuming non-empty and rectangular)\n",
    "print(\"X dimensions:\", (num_rows, num_columns)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y: (198982, 1400)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Read label data from files (assuming you have already defined this part)\n",
    "label_data = []\n",
    "file_names = ['icd_codes_1.txt', 'icd_codes_2.txt']  # Update with actual filenames\n",
    "for file_name in file_names:\n",
    "    with open(file_name, 'r') as file:\n",
    "        label_data.extend(line.strip() for line in file if line.strip())\n",
    "\n",
    "# Step 2: Create a set of unique ICD-10 codes for efficient lookup\n",
    "unique_codes = set()\n",
    "for labels in label_data:\n",
    "    unique_codes.update(labels.split(\";\"))\n",
    "unique_codes = sorted(unique_codes)  # Convert to a sorted list at the end\n",
    "\n",
    "# Step 3: Initialize the StringLookup layer\n",
    "lookup_layer = tf.keras.layers.StringLookup(vocabulary=unique_codes, output_mode=\"multi_hot\", mask_token=None,num_oov_indices=0)\n",
    "\n",
    "# Step 4: Create a tf.data.Dataset to handle large data efficiently\n",
    "label_data_ds = tf.data.Dataset.from_tensor_slices(label_data)\n",
    "\n",
    "# Step 5: Define a function to encode each label set\n",
    "def encode_labels(labels):\n",
    "    return lookup_layer(tf.strings.split(labels, sep=\";\"))\n",
    "\n",
    "# Step 6: Map encoding function over the dataset and batch it\n",
    "# Batch processing reduces memory usage\n",
    "multi_hot_labels_ds = label_data_ds.map(encode_labels, num_parallel_calls=tf.data.AUTOTUNE).batch(1000)\n",
    "\n",
    "# Step 7: Concatenate all batches to get the final `y` tensor\n",
    "y = tf.concat(list(multi_hot_labels_ds), axis=0)\n",
    "\n",
    "# Ensure the correct shape of `y`\n",
    "print(\"Shape of y:\", y.shape)  # Should output: (200000, 1400)\n",
    "\n",
    "y = y.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (198982, 1024)\n",
      "Shape of y: (198982, 1400)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (159185, 1024)\n",
      "y_train shape: (159185, 1400)\n",
      "X_test shape: (39797, 1024)\n",
      "y_test shape: (39797, 1400)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_val.shape)\n",
    "print(\"y_test shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MicroF2Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='micro_f2_score', beta=2, **kwargs):\n",
    "        super(MicroF2Score, self).__init__(name=name, **kwargs)\n",
    "        self.beta = beta\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Threshold y_pred to get binary predictions\n",
    "        y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "        \n",
    "        # Cast y_true to float32 to ensure compatibility\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        true_positive = tf.reduce_sum(y_true * y_pred)\n",
    "        false_positive = tf.reduce_sum(y_pred * (1 - y_true))\n",
    "        false_negative = tf.reduce_sum((1 - y_pred) * y_true)\n",
    "\n",
    "        # Update the corresponding weights\n",
    "        self.tp.assign_add(true_positive)\n",
    "        self.fp.assign_add(false_positive)\n",
    "        self.fn.assign_add(false_negative)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
    "        f_beta = (1 + self.beta**2) * (precision * recall) / (self.beta**2 * precision + recall + tf.keras.backend.epsilon())\n",
    "        return f_beta\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        self.fn.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Focal Loss for handling label imbalance\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7)\n",
    "        pos_loss = -alpha * tf.pow(1 - y_pred, gamma) * y_true * tf.math.log(y_pred)\n",
    "        neg_loss = -(1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(pos_loss + neg_loss)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Enhanced model architecture with larger layers and increased dropout\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(2048, activation='relu'),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1024, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(1400, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "model = create_model(X_train.shape[1])\n",
    "\n",
    "# Compile model with Adam optimizer and custom focal loss\n",
    "optimizer = Adam(learning_rate=0.0003)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[MicroF2Score()])\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, mode='min')\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=32,  # Optimized batch size\n",
    "                    epochs=100,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, lr_scheduler],\n",
    "                    verbose=1)\n",
    "\n",
    "# Function to find optimal threshold for each label\n",
    "def tune_thresholds(y_true, y_pred, beta=2.0, num_thresholds=50):\n",
    "    best_thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        best_f2 = 0\n",
    "        best_threshold = 0.5\n",
    "        for threshold in np.linspace(0.1, 0.9, num_thresholds):\n",
    "            y_pred_bin = (y_pred[:, i] >= threshold).astype(int)\n",
    "            f2 = fbeta_score(y_true[:, i], y_pred_bin, beta=beta, average='micro', zero_division=1)\n",
    "            if f2 > best_f2:\n",
    "                best_f2 = f2\n",
    "                best_threshold = threshold\n",
    "        best_thresholds.append(best_threshold)\n",
    "    return best_thresholds\n",
    "\n",
    "# Get predictions on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Find optimal thresholds for each label\n",
    "best_thresholds = tune_thresholds(y_val, y_val_pred)\n",
    "\n",
    "# Apply optimized thresholds to validation predictions\n",
    "y_val_pred_bin = np.array([(y_val_pred[:, i] >= best_thresholds[i]).astype(int) for i in range(y_val_pred.shape[1])]).T\n",
    "\n",
    "# Calculate and print the validation F2 score with optimized thresholds\n",
    "validation_f2 = fbeta_score(y_val, y_val_pred_bin, beta=2, average='micro', zero_division=1)\n",
    "print(\"Optimized Micro-F2 Score on validation set:\", validation_f2)\n",
    "\n",
    "# Inference function with optimized thresholds for test data\n",
    "def predict_with_thresholds(model, X, thresholds):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_bin = np.array([(y_pred[:, i] >= thresholds[i]).astype(int) for i in range(y_pred.shape[1])]).T\n",
    "    return y_pred_bin\n",
    "\n",
    "# Example usage for test set predictions\n",
    "# y_test_pred = predict_with_thresholds(model, X_test, best_thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - loss: 0.2161 - micro_f2_score: 0.0218 - val_loss: 0.0053 - val_micro_f2_score: 0.3289 - learning_rate: 5.0000e-04\n",
      "Epoch 2/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - loss: 0.0057 - micro_f2_score: 0.3030 - val_loss: 0.0034 - val_micro_f2_score: 0.5288 - learning_rate: 5.0000e-04\n",
      "Epoch 3/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - loss: 0.0040 - micro_f2_score: 0.4505 - val_loss: 0.0025 - val_micro_f2_score: 0.6488 - learning_rate: 5.0000e-04\n",
      "Epoch 4/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - loss: 0.0032 - micro_f2_score: 0.5452 - val_loss: 0.0022 - val_micro_f2_score: 0.6946 - learning_rate: 5.0000e-04\n",
      "Epoch 5/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - loss: 0.0028 - micro_f2_score: 0.6003 - val_loss: 0.0020 - val_micro_f2_score: 0.7330 - learning_rate: 5.0000e-04\n",
      "Epoch 6/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0025 - micro_f2_score: 0.6357 - val_loss: 0.0019 - val_micro_f2_score: 0.7416 - learning_rate: 5.0000e-04\n",
      "Epoch 7/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0024 - micro_f2_score: 0.6564 - val_loss: 0.0018 - val_micro_f2_score: 0.7510 - learning_rate: 5.0000e-04\n",
      "Epoch 8/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0023 - micro_f2_score: 0.6722 - val_loss: 0.0018 - val_micro_f2_score: 0.7637 - learning_rate: 5.0000e-04\n",
      "Epoch 9/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0022 - micro_f2_score: 0.6874 - val_loss: 0.0017 - val_micro_f2_score: 0.7675 - learning_rate: 5.0000e-04\n",
      "Epoch 10/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0021 - micro_f2_score: 0.6971 - val_loss: 0.0017 - val_micro_f2_score: 0.7757 - learning_rate: 5.0000e-04\n",
      "Epoch 11/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0021 - micro_f2_score: 0.7039 - val_loss: 0.0017 - val_micro_f2_score: 0.7812 - learning_rate: 5.0000e-04\n",
      "Epoch 12/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0020 - micro_f2_score: 0.7088 - val_loss: 0.0016 - val_micro_f2_score: 0.7809 - learning_rate: 5.0000e-04\n",
      "Epoch 13/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0020 - micro_f2_score: 0.7152 - val_loss: 0.0017 - val_micro_f2_score: 0.7865 - learning_rate: 5.0000e-04\n",
      "Epoch 14/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0020 - micro_f2_score: 0.7213 - val_loss: 0.0016 - val_micro_f2_score: 0.7890 - learning_rate: 5.0000e-04\n",
      "Epoch 15/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0019 - micro_f2_score: 0.7238 - val_loss: 0.0016 - val_micro_f2_score: 0.7911 - learning_rate: 5.0000e-04\n",
      "Epoch 16/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0019 - micro_f2_score: 0.7283 - val_loss: 0.0016 - val_micro_f2_score: 0.7935 - learning_rate: 5.0000e-04\n",
      "Epoch 17/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - loss: 0.0019 - micro_f2_score: 0.7293 - val_loss: 0.0016 - val_micro_f2_score: 0.7918 - learning_rate: 5.0000e-04\n",
      "Epoch 18/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0018 - micro_f2_score: 0.7380 - val_loss: 0.0016 - val_micro_f2_score: 0.7935 - learning_rate: 2.5000e-04\n",
      "Epoch 19/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0018 - micro_f2_score: 0.7433 - val_loss: 0.0015 - val_micro_f2_score: 0.7992 - learning_rate: 2.5000e-04\n",
      "Epoch 20/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - loss: 0.0018 - micro_f2_score: 0.7473 - val_loss: 0.0015 - val_micro_f2_score: 0.7987 - learning_rate: 2.5000e-04\n",
      "Epoch 21/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0017 - micro_f2_score: 0.7503 - val_loss: 0.0015 - val_micro_f2_score: 0.7983 - learning_rate: 2.5000e-04\n",
      "Epoch 22/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0017 - micro_f2_score: 0.7533 - val_loss: 0.0015 - val_micro_f2_score: 0.8025 - learning_rate: 2.5000e-04\n",
      "Epoch 23/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0017 - micro_f2_score: 0.7531 - val_loss: 0.0015 - val_micro_f2_score: 0.8014 - learning_rate: 2.5000e-04\n",
      "Epoch 24/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0017 - micro_f2_score: 0.7570 - val_loss: 0.0015 - val_micro_f2_score: 0.8015 - learning_rate: 2.5000e-04\n",
      "Epoch 25/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0017 - micro_f2_score: 0.7579 - val_loss: 0.0015 - val_micro_f2_score: 0.8027 - learning_rate: 2.5000e-04\n",
      "Epoch 26/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0017 - micro_f2_score: 0.7612 - val_loss: 0.0015 - val_micro_f2_score: 0.8055 - learning_rate: 1.2500e-04\n",
      "Epoch 27/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0017 - micro_f2_score: 0.7639 - val_loss: 0.0015 - val_micro_f2_score: 0.8066 - learning_rate: 1.2500e-04\n",
      "Epoch 28/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7657 - val_loss: 0.0015 - val_micro_f2_score: 0.8078 - learning_rate: 1.2500e-04\n",
      "Epoch 29/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7653 - val_loss: 0.0015 - val_micro_f2_score: 0.8084 - learning_rate: 1.2500e-04\n",
      "Epoch 30/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7679 - val_loss: 0.0015 - val_micro_f2_score: 0.8073 - learning_rate: 1.2500e-04\n",
      "Epoch 31/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7680 - val_loss: 0.0015 - val_micro_f2_score: 0.8088 - learning_rate: 6.2500e-05\n",
      "Epoch 32/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - loss: 0.0016 - micro_f2_score: 0.7705 - val_loss: 0.0015 - val_micro_f2_score: 0.8085 - learning_rate: 6.2500e-05\n",
      "Epoch 33/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7699 - val_loss: 0.0015 - val_micro_f2_score: 0.8097 - learning_rate: 6.2500e-05\n",
      "Epoch 34/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7717 - val_loss: 0.0015 - val_micro_f2_score: 0.8086 - learning_rate: 6.2500e-05\n",
      "Epoch 35/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7724 - val_loss: 0.0015 - val_micro_f2_score: 0.8091 - learning_rate: 6.2500e-05\n",
      "Epoch 36/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7726 - val_loss: 0.0015 - val_micro_f2_score: 0.8088 - learning_rate: 3.1250e-05\n",
      "Epoch 37/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7738 - val_loss: 0.0015 - val_micro_f2_score: 0.8094 - learning_rate: 3.1250e-05\n",
      "Epoch 38/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7752 - val_loss: 0.0015 - val_micro_f2_score: 0.8098 - learning_rate: 3.1250e-05\n",
      "Epoch 39/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7746 - val_loss: 0.0015 - val_micro_f2_score: 0.8104 - learning_rate: 3.1250e-05\n",
      "Epoch 40/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7756 - val_loss: 0.0015 - val_micro_f2_score: 0.8096 - learning_rate: 3.1250e-05\n",
      "Epoch 41/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7751 - val_loss: 0.0015 - val_micro_f2_score: 0.8099 - learning_rate: 1.5625e-05\n",
      "Epoch 42/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7748 - val_loss: 0.0015 - val_micro_f2_score: 0.8112 - learning_rate: 1.5625e-05\n",
      "Epoch 43/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7739 - val_loss: 0.0015 - val_micro_f2_score: 0.8107 - learning_rate: 1.5625e-05\n",
      "Epoch 44/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7756 - val_loss: 0.0015 - val_micro_f2_score: 0.8108 - learning_rate: 1.5625e-05\n",
      "Epoch 45/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7754 - val_loss: 0.0015 - val_micro_f2_score: 0.8109 - learning_rate: 1.5625e-05\n",
      "Epoch 46/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7769 - val_loss: 0.0015 - val_micro_f2_score: 0.8116 - learning_rate: 7.8125e-06\n",
      "Epoch 47/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7754 - val_loss: 0.0015 - val_micro_f2_score: 0.8116 - learning_rate: 7.8125e-06\n",
      "Epoch 48/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7769 - val_loss: 0.0015 - val_micro_f2_score: 0.8114 - learning_rate: 7.8125e-06\n",
      "Epoch 49/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7771 - val_loss: 0.0015 - val_micro_f2_score: 0.8104 - learning_rate: 7.8125e-06\n",
      "Epoch 50/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7775 - val_loss: 0.0015 - val_micro_f2_score: 0.8111 - learning_rate: 7.8125e-06\n",
      "Epoch 51/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - loss: 0.0016 - micro_f2_score: 0.7765 - val_loss: 0.0015 - val_micro_f2_score: 0.8113 - learning_rate: 3.9063e-06\n",
      "Epoch 52/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7757 - val_loss: 0.0015 - val_micro_f2_score: 0.8113 - learning_rate: 3.9063e-06\n",
      "Epoch 53/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7780 - val_loss: 0.0015 - val_micro_f2_score: 0.8102 - learning_rate: 3.9063e-06\n",
      "Epoch 54/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7774 - val_loss: 0.0015 - val_micro_f2_score: 0.8110 - learning_rate: 3.9063e-06\n",
      "Epoch 55/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7765 - val_loss: 0.0015 - val_micro_f2_score: 0.8112 - learning_rate: 3.9063e-06\n",
      "Epoch 56/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7763 - val_loss: 0.0015 - val_micro_f2_score: 0.8105 - learning_rate: 1.9531e-06\n",
      "Epoch 57/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7784 - val_loss: 0.0015 - val_micro_f2_score: 0.8108 - learning_rate: 1.9531e-06\n",
      "Epoch 58/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7768 - val_loss: 0.0015 - val_micro_f2_score: 0.8120 - learning_rate: 1.9531e-06\n",
      "Epoch 59/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7783 - val_loss: 0.0015 - val_micro_f2_score: 0.8113 - learning_rate: 1.9531e-06\n",
      "Epoch 60/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7757 - val_loss: 0.0015 - val_micro_f2_score: 0.8115 - learning_rate: 1.9531e-06\n",
      "Epoch 61/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7777 - val_loss: 0.0015 - val_micro_f2_score: 0.8112 - learning_rate: 9.7656e-07\n",
      "Epoch 62/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0016 - micro_f2_score: 0.7763 - val_loss: 0.0015 - val_micro_f2_score: 0.8103 - learning_rate: 9.7656e-07\n",
      "Epoch 63/70\n",
      "\u001b[1m2239/2239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - loss: 0.0015 - micro_f2_score: 0.7776 - val_loss: 0.0015 - val_micro_f2_score: 0.8116 - learning_rate: 9.7656e-07\n",
      "\u001b[1m1244/1244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "Optimized Micro-F2 Score on validation set: 0.8184692296688884\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, LeakyReLU\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import fbeta_score\n",
    "\n",
    "# # Define the model using the Sequential API with added complexity and batch normalization\n",
    "# def create_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=(input_shape,)),  # Specify the input shape directly\n",
    "#         Dense(1024, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.4),  # Increased dropout rate for better regularization\n",
    "#         Dense(512, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.4),\n",
    "#         Dense(256, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.4),\n",
    "#         Dense(1400, activation='sigmoid')  # Sigmoid activation for multi-label classification\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# # Create the model\n",
    "# model = create_model(X_train.shape[1])\n",
    "\n",
    "# # Compile the model with Adam optimizer, focal loss, and custom F2Score metric\n",
    "# optimizer = Adam(learning_rate=0.0005)\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[MicroF2Score()])\n",
    "\n",
    "# # EarlyStopping to monitor the validation F2 score, aiming to stop training when F2 score stops improving\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
    "\n",
    "# # ModelCheckpoint to save the model weights only when the validation F2 score improves\n",
    "# model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# # Learning rate scheduler to reduce the learning rate when validation loss plateaus\n",
    "# lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, mode='min')\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# history = model.fit(X_train, y_train, \n",
    "#                     batch_size=32,  # Increased batch size for stability\n",
    "#                     epochs=70,      # Increased number of epochs\n",
    "#                     validation_split=0.1,  # Use part of the training set for validation\n",
    "#                     callbacks=[early_stopping, model_checkpoint, lr_scheduler],\n",
    "#                     verbose=1)\n",
    "\n",
    "# # Post-training: Optimize threshold for F2-score\n",
    "# def tune_thresholds(y_true, y_pred, beta=2.0, num_thresholds=50):\n",
    "#     best_thresholds = []\n",
    "#     for i in range(y_true.shape[1]):\n",
    "#         best_f2 = 0\n",
    "#         best_threshold = 0.5\n",
    "#         for threshold in np.linspace(0.1, 0.9, num_thresholds):\n",
    "#             y_pred_bin = (y_pred[:, i] >= threshold).astype(int)\n",
    "#             f2 = fbeta_score(y_true[:, i], y_pred_bin, beta=beta, average='micro', zero_division=1)\n",
    "#             if f2 > best_f2:\n",
    "#                 best_f2 = f2\n",
    "#                 best_threshold = threshold\n",
    "#         best_thresholds.append(best_threshold)\n",
    "#     return best_thresholds\n",
    "\n",
    "# # Get predictions on validation set\n",
    "# y_val_pred = model.predict(X_val)\n",
    "\n",
    "# # Find optimal thresholds for each label\n",
    "# best_thresholds = tune_thresholds(y_val, y_val_pred)\n",
    "\n",
    "# # Apply thresholds to validation predictions\n",
    "# y_val_pred_bin = (y_val_pred >= best_thresholds).astype(int)\n",
    "\n",
    "# # Calculate validation F2 score with optimized thresholds\n",
    "# validation_f2 = fbeta_score(y_val, y_val_pred_bin, beta=2, average='micro', zero_division=1)\n",
    "# print(\"Optimized Micro-F2 Score on validation set:\", validation_f2)\n",
    "\n",
    "# # Inference function\n",
    "# def predict_with_thresholds(model, X, thresholds):\n",
    "#     y_pred = model.predict(X)\n",
    "#     y_pred_bin = (y_pred >= thresholds).astype(int)\n",
    "#     return y_pred_bin\n",
    "\n",
    "# # Example usage for test set\n",
    "# # y_test_pred = predict_with_thresholds(model, X_test, best_thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, LeakyReLU\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.metrics import fbeta_score\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the model using the Sequential API with added complexity and batch normalization\n",
    "# model = Sequential([\n",
    "#     Input(shape=(X_train.shape[1],)),  # Specify the input shape directly\n",
    "#     Dense(1024, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(1400, activation='sigmoid')  # Sigmoid activation for multi-label classification\n",
    "# ])\n",
    "\n",
    "# # Compile the model with Adam optimizer and binary crossentropy loss\n",
    "# optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# class F2Score(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, name='f2_score', beta=2, **kwargs):\n",
    "#         super(F2Score, self).__init__(name=name, **kwargs)\n",
    "#         self.beta = beta\n",
    "#         self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "#         self.actual_positive = self.add_weight(name='actual_positive', initializer='zeros')\n",
    "#         self.predicted_positive = self.add_weight(name='predicted_positive', initializer='zeros')\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_true = tf.cast(y_true, tf.bool)\n",
    "#         y_pred = tf.cast(y_pred > 0.5, tf.bool)  # Threshold can be adjusted\n",
    "\n",
    "#         true_positive = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "#         self.tp.assign_add(tf.reduce_sum(tf.cast(true_positive, self.dtype)))\n",
    "#         self.actual_positive.assign_add(tf.reduce_sum(tf.cast(y_true, self.dtype)))\n",
    "#         self.predicted_positive.assign_add(tf.reduce_sum(tf.cast(y_pred, self.dtype)))\n",
    "\n",
    "#     def result(self):\n",
    "#         precision = self.tp / (self.predicted_positive + tf.keras.backend.epsilon())\n",
    "#         recall = self.tp / (self.actual_positive + tf.keras.backend.epsilon())\n",
    "#         f_beta = (1 + self.beta**2) * (precision * recall) / (self.beta**2 * precision + recall + tf.keras.backend.epsilon())\n",
    "#         return f_beta\n",
    "\n",
    "#     def reset_states(self):\n",
    "#         self.tp.assign(0)\n",
    "#         self.actual_positive.assign(0)\n",
    "#         self.predicted_positive.assign(0)\n",
    "\n",
    "# # Use F2Score in model compilation\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[F2Score()])\n",
    "\n",
    "# # EarlyStopping to monitor the validation F2 score, aiming to stop training when F2 score stops improving\n",
    "# early_stopping = EarlyStopping(monitor='val_f2_score', patience=10, restore_best_weights=True, mode='max')\n",
    "\n",
    "# # ModelCheckpoint to save the model weights only when the validation F2 score improves\n",
    "# model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_f2_score', mode='max')\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# history = model.fit(X_train, y_train, \n",
    "#                     batch_size=32,  # Batch size can be adjusted based on system capabilities\n",
    "#                     epochs=100,      # Increased number of epochs\n",
    "#                     validation_split=0.1,  # Use part of the training set for validation\n",
    "#                     callbacks=[early_stopping, model_checkpoint],\n",
    "#                     verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3110/3110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load('test_data.npy')\n",
    "\n",
    "# Predict probabilities and convert to binary\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Prepare reverse lookup\n",
    "lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_codes, invert=True, output_mode=\"int\", mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "# Convert predictions to ICD10 codes\n",
    "predicted_indices = [np.where(pred_row == 1)[0] for pred_row in y_pred]\n",
    "predicted_codes = [lookup_layer(indices).numpy() for indices in predicted_indices]\n",
    "predicted_codes = [[code.decode('utf-8') for code in row] for row in predicted_codes]\n",
    "predicted_labels = [';'.join(row) for row in predicted_codes]\n",
    "\n",
    "# Create and save submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(1, len(predicted_labels) + 1),\n",
    "    'labels': predicted_labels\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3110/3110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load('test_data.npy')\n",
    "\n",
    "# Predict probabilities and convert to binary using the tuned thresholds\n",
    "y_test_pred = predict_with_thresholds(model, X_test, best_thresholds)\n",
    "\n",
    "# Prepare reverse lookup\n",
    "lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_codes, invert=True, output_mode=\"int\", mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "# Convert predictions to ICD10 codes\n",
    "predicted_indices = [np.where(pred_row == 1)[0] for pred_row in y_test_pred]\n",
    "predicted_codes = [lookup_layer(indices).numpy() for indices in predicted_indices]\n",
    "predicted_codes = [[code.decode('utf-8') for code in row] for row in predicted_codes]\n",
    "predicted_labels = [';'.join(row) for row in predicted_codes]\n",
    "\n",
    "# Create and save submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(1, len(predicted_labels) + 1),\n",
    "    'labels': predicted_labels\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
